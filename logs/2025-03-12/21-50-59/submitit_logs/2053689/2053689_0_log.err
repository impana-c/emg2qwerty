2025-03-12 21:51:19.604704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741841479.625977 2053743 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741841479.632613 2053743 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-12 21:51:19.655658: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Global seed set to 1501
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/bkwan27/emg2qwerty/logs/2025-03-12/21-50-59/job0_trainer.devices=1,user=single_user/lightning_logs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html
  "lr_options": generate_power_seq(LEARNING_RATE_CIFAR, 11),
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html
  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask("01, 02, 11"),
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html
  self.nce_loss = AmdimNCELoss(tclip)
/home/bkwan27/anaconda3/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html
  return _target_(*args, **kwargs)

  | Name                | Type               | Params
-----------------------------------------------------------
0 | tokenizer           | Sequential         | 406 K 
1 | positional_encoding | PositionalEncoding | 0     
2 | model               | Transformer        | 11.1 M
3 | softmax             | LogSoftmax         | 0     
4 | ctc_loss            | CTCLoss            | 0     
5 | metrics             | ModuleDict         | 0     
-----------------------------------------------------------
11.5 M    Trainable params
0         Non-trainable params
11.5 M    Total params
45.964    Total estimated model params size (MB)
Epoch 0, global step 120: 'val/CER' reached 2311.65259 (best 2311.65259), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-12/21-50-59/job0_trainer.devices=1,user=single_user/checkpoints/epoch=0-step=120.ckpt' as top 1
Epoch 1, global step 240: 'val/CER' reached 100.00000 (best 100.00000), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-12/21-50-59/job0_trainer.devices=1,user=single_user/checkpoints/epoch=1-step=240.ckpt' as top 1
Epoch 2, global step 360: 'val/CER' was not in top 1
Epoch 3, global step 480: 'val/CER' was not in top 1
Epoch 4, global step 600: 'val/CER' was not in top 1
Epoch 5, global step 720: 'val/CER' was not in top 1
Epoch 6, global step 840: 'val/CER' was not in top 1
Epoch 7, global step 960: 'val/CER' was not in top 1
Epoch 8, global step 1080: 'val/CER' was not in top 1
Epoch 9, global step 1200: 'val/CER' was not in top 1
Epoch 10, global step 1320: 'val/CER' was not in top 1
Epoch 11, global step 1440: 'val/CER' was not in top 1
Epoch 12, global step 1560: 'val/CER' was not in top 1
Epoch 13, global step 1680: 'val/CER' was not in top 1
Epoch 14, global step 1800: 'val/CER' was not in top 1
Epoch 15, global step 1920: 'val/CER' was not in top 1
Epoch 16, global step 2040: 'val/CER' was not in top 1
Epoch 17, global step 2160: 'val/CER' reached 99.95570 (best 99.95570), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-12/21-50-59/job0_trainer.devices=1,user=single_user/checkpoints/epoch=17-step=2160.ckpt' as top 1
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
Traceback (most recent call last):
  File "/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 645, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1098, in _run
    results = self._run_stage()
  File "/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1177, in _run_stage
    self._run_train()
  File "/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1200, in _run_train
    self.fit_loop.run()
  File "/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.on_advance_end()
  File "/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 295, in on_advance_end
    self.trainer._call_callback_hooks("on_train_epoch_end")
  File "/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1380, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 312, in on_train_epoch_end
    self._save_topk_checkpoint(trainer, monitor_candidates)
  File "/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 369, in _save_topk_checkpoint
    