2025-03-09 19:29:16.641407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741573756.653214  261698 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741573756.656511  261698 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-09 19:29:16.668798: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Global seed set to 1501
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2025-03-09 19:29:22.875262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Att2025-03-09 19:29:22.875263: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
empting to register factory for plugin cuFFT when one has already been registered
2025-03-09 19:29:22.877444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741573762.885956  261824 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741573762.885956  261826 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741573762.888652  261825 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741573762.889292  261826 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741573762.889298  261824 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741573762.891980  261825 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-09 19:29:22.902643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-09 19:29:22.902686: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-09 19:29:22.905191: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/3
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/3
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/3
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 3 processes
----------------------------------------------------------------------------------------------------

Missing logger folder: /home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/lightning_logs
Missing logger folder: /home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/lightning_logs
Missing logger folder: /home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/lightning_logs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2]
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html
  "lr_options": generate_power_seq(LEARNING_RATE_CIFAR, 11),
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html
  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask("01, 02, 11"),
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html
  self.nce_loss = AmdimNCELoss(tclip)
/home/bkwan27/anaconda3/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html
  return _target_(*args, **kwargs)

  | Name     | Type       | Params
----------------------------------------
0 | model    | Sequential | 5.3 M 
1 | ctc_loss | CTCLoss    | 0     
2 | metrics  | ModuleDict | 0     
----------------------------------------
5.3 M     Trainable params
0         Non-trainable params
5.3 M     Total params
21.173    Total estimated model params size (MB)
/home/bkwan27/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/bkwan27/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/bkwan27/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
[rank2]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/home/bkwan27/anaconda3/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/bkwan27/anaconda3/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/bkwan27/anaconda3/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 0, global step 40: 'val/CER' reached 1356.46875 (best 1356.46875), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=0-step=40.ckpt' as top 1
Epoch 1, global step 80: 'val/CER' reached 100.00000 (best 100.00000), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=1-step=80.ckpt' as top 1
Epoch 2, global step 120: 'val/CER' was not in top 1
Epoch 3, global step 160: 'val/CER' was not in top 1
Epoch 4, global step 200: 'val/CER' was not in top 1
Epoch 5, global step 240: 'val/CER' was not in top 1
Epoch 6, global step 280: 'val/CER' was not in top 1
Epoch 7, global step 320: 'val/CER' was not in top 1
Epoch 8, global step 360: 'val/CER' reached 99.95570 (best 99.95570), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=8-step=360.ckpt' as top 1
Epoch 9, global step 400: 'val/CER' was not in top 1
Epoch 10, global step 440: 'val/CER' reached 99.88924 (best 99.88924), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=10-step=440.ckpt' as top 1
Epoch 11, global step 480: 'val/CER' was not in top 1
Epoch 12, global step 520: 'val/CER' reached 99.75632 (best 99.75632), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=12-step=520.ckpt' as top 1
Epoch 13, global step 560: 'val/CER' was not in top 1
Epoch 14, global step 600: 'val/CER' reached 99.57909 (best 99.57909), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=14-step=600.ckpt' as top 1
Epoch 15, global step 640: 'val/CER' was not in top 1
Epoch 16, global step 680: 'val/CER' reached 97.45238 (best 97.45238), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=16-step=680.ckpt' as top 1
Epoch 17, global step 720: 'val/CER' reached 93.48693 (best 93.48693), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=17-step=720.ckpt' as top 1
Epoch 18, global step 760: 'val/CER' reached 89.27780 (best 89.27780), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=18-step=760.ckpt' as top 1
Epoch 19, global step 800: 'val/CER' reached 86.73018 (best 86.73018), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=19-step=800.ckpt' as top 1
Epoch 20, global step 840: 'val/CER' reached 85.02437 (best 85.02437), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=20-step=840.ckpt' as top 1
Epoch 21, global step 880: 'val/CER' reached 84.82500 (best 84.82500), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=21-step=880.ckpt' as top 1
Epoch 22, global step 920: 'val/CER' reached 80.94817 (best 80.94817), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=22-step=920.ckpt' as top 1
Epoch 23, global step 960: 'val/CER' reached 79.57466 (best 79.57466), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=23-step=960.ckpt' as top 1
Epoch 24, global step 1000: 'val/CER' reached 75.49846 (best 75.49846), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=24-step=1000.ckpt' as top 1
Epoch 25, global step 1040: 'val/CER' reached 72.17546 (best 72.17546), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=25-step=1040.ckpt' as top 1
Epoch 26, global step 1080: 'val/CER' reached 67.94418 (best 67.94418), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=26-step=1080.ckpt' as top 1
Epoch 27, global step 1120: 'val/CER' was not in top 1
Epoch 28, global step 1160: 'val/CER' reached 64.73195 (best 64.73195), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=28-step=1160.ckpt' as top 1
Epoch 29, global step 1200: 'val/CER' reached 62.71600 (best 62.71600), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=29-step=1200.ckpt' as top 1
Epoch 30, global step 1240: 'val/CER' reached 59.72530 (best 59.72530), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=30-step=1240.ckpt' as top 1
Epoch 31, global step 1280: 'val/CER' reached 52.01595 (best 52.01595), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=31-step=1280.ckpt' as top 1
Epoch 32, global step 1320: 'val/CER' reached 39.80949 (best 39.80949), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=32-step=1320.ckpt' as top 1
Epoch 33, global step 1360: 'val/CER' reached 38.10368 (best 38.10368), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=33-step=1360.ckpt' as top 1
Epoch 34, global step 1400: 'val/CER' reached 36.75233 (best 36.75233), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=34-step=1400.ckpt' as top 1
Epoch 35, global step 1440: 'val/CER' reached 35.53390 (best 35.53390), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=35-step=1440.ckpt' as top 1
Epoch 36, global step 1480: 'val/CER' reached 34.91360 (best 34.91360), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=36-step=1480.ckpt' as top 1
Epoch 37, global step 1520: 'val/CER' reached 33.42933 (best 33.42933), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=37-step=1520.ckpt' as top 1
Epoch 38, global step 1560: 'val/CER' was not in top 1
Epoch 39, global step 1600: 'val/CER' reached 32.23306 (best 32.23306), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=39-step=1600.ckpt' as top 1
Epoch 40, global step 1640: 'val/CER' was not in top 1
Epoch 41, global step 1680: 'val/CER' reached 31.78999 (best 31.78999), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=41-step=1680.ckpt' as top 1
Epoch 42, global step 1720: 'val/CER' reached 29.28666 (best 29.28666), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=42-step=1720.ckpt' as top 1
Epoch 43, global step 1760: 'val/CER' was not in top 1
Epoch 44, global step 1800: 'val/CER' was not in top 1
Epoch 45, global step 1840: 'val/CER' was not in top 1
Epoch 46, global step 1880: 'val/CER' reached 28.20115 (best 28.20115), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=46-step=1880.ckpt' as top 1
Epoch 47, global step 1920: 'val/CER' reached 26.76119 (best 26.76119), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=47-step=1920.ckpt' as top 1
Epoch 48, global step 1960: 'val/CER' reached 25.43199 (best 25.43199), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=48-step=1960.ckpt' as top 1
Epoch 49, global step 2000: 'val/CER' was not in top 1
Epoch 50, global step 2040: 'val/CER' was not in top 1
Epoch 51, global step 2080: 'val/CER' reached 25.25476 (best 25.25476), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=51-step=2080.ckpt' as top 1
Epoch 52, global step 2120: 'val/CER' was not in top 1
Epoch 53, global step 2160: 'val/CER' reached 24.87816 (best 24.87816), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=53-step=2160.ckpt' as top 1
Epoch 54, global step 2200: 'val/CER' reached 24.39078 (best 24.39078), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=54-step=2200.ckpt' as top 1
Epoch 55, global step 2240: 'val/CER' reached 24.10279 (best 24.10279), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=55-step=2240.ckpt' as top 1
Epoch 56, global step 2280: 'val/CER' was not in top 1
Epoch 57, global step 2320: 'val/CER' was not in top 1
Epoch 58, global step 2360: 'val/CER' reached 22.66283 (best 22.66283), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=58-step=2360.ckpt' as top 1
Epoch 59, global step 2400: 'val/CER' was not in top 1
Epoch 60, global step 2440: 'val/CER' was not in top 1
Epoch 61, global step 2480: 'val/CER' was not in top 1
Epoch 62, global step 2520: 'val/CER' was not in top 1
Epoch 63, global step 2560: 'val/CER' was not in top 1
Epoch 64, global step 2600: 'val/CER' was not in top 1
Epoch 65, global step 2640: 'val/CER' was not in top 1
Epoch 66, global step 2680: 'val/CER' reached 22.64067 (best 22.64067), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=66-step=2680.ckpt' as top 1
Epoch 67, global step 2720: 'val/CER' was not in top 1
Epoch 68, global step 2760: 'val/CER' reached 22.33053 (best 22.33053), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=68-step=2760.ckpt' as top 1
Epoch 69, global step 2800: 'val/CER' was not in top 1
Epoch 70, global step 2840: 'val/CER' reached 22.15330 (best 22.15330), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=70-step=2840.ckpt' as top 1
Epoch 71, global step 2880: 'val/CER' was not in top 1
Epoch 72, global step 2920: 'val/CER' reached 22.06469 (best 22.06469), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=72-step=2920.ckpt' as top 1
Epoch 73, global step 2960: 'val/CER' was not in top 1
Epoch 74, global step 3000: 'val/CER' was not in top 1
Epoch 75, global step 3040: 'val/CER' reached 21.51085 (best 21.51085), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=75-step=3040.ckpt' as top 1
Epoch 76, global step 3080: 'val/CER' was not in top 1
Epoch 77, global step 3120: 'val/CER' reached 21.20071 (best 21.20071), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=77-step=3120.ckpt' as top 1
Epoch 78, global step 3160: 'val/CER' was not in top 1
Epoch 79, global step 3200: 'val/CER' was not in top 1
Epoch 80, global step 3240: 'val/CER' was not in top 1
Epoch 81, global step 3280: 'val/CER' was not in top 1
Epoch 82, global step 3320: 'val/CER' was not in top 1
Epoch 83, global step 3360: 'val/CER' was not in top 1
Epoch 84, global step 3400: 'val/CER' reached 20.91272 (best 20.91272), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=84-step=3400.ckpt' as top 1
Epoch 85, global step 3440: 'val/CER' was not in top 1
Epoch 86, global step 3480: 'val/CER' was not in top 1
Epoch 87, global step 3520: 'val/CER' was not in top 1
Epoch 88, global step 3560: 'val/CER' reached 20.86841 (best 20.86841), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=88-step=3560.ckpt' as top 1
Epoch 89, global step 3600: 'val/CER' was not in top 1
Epoch 90, global step 3640: 'val/CER' was not in top 1
Epoch 91, global step 3680: 'val/CER' was not in top 1
Epoch 92, global step 3720: 'val/CER' was not in top 1
Epoch 93, global step 3760: 'val/CER' was not in top 1
Epoch 94, global step 3800: 'val/CER' was not in top 1
Epoch 95, global step 3840: 'val/CER' reached 20.80195 (best 20.80195), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=95-step=3840.ckpt' as top 1
Epoch 96, global step 3880: 'val/CER' reached 20.33673 (best 20.33673), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=96-step=3880.ckpt' as top 1
Epoch 97, global step 3920: 'val/CER' was not in top 1
Epoch 98, global step 3960: 'val/CER' was not in top 1
Epoch 99, global step 4000: 'val/CER' was not in top 1
Epoch 100, global step 4040: 'val/CER' was not in top 1
Epoch 101, global step 4080: 'val/CER' was not in top 1
Epoch 102, global step 4120: 'val/CER' reached 20.29242 (best 20.29242), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=102-step=4120.ckpt' as top 1
Epoch 103, global step 4160: 'val/CER' was not in top 1
Epoch 104, global step 4200: 'val/CER' was not in top 1
Epoch 105, global step 4240: 'val/CER' was not in top 1
Epoch 106, global step 4280: 'val/CER' was not in top 1
Epoch 107, global step 4320: 'val/CER' was not in top 1
Epoch 108, global step 4360: 'val/CER' reached 19.98228 (best 19.98228), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/19-29-13/job0_trainer.devices=3,user=single_user/checkpoints/epoch=108-step=4360.ckpt' as top 1
Epoch 109, global step 4400: 'val/CER' was not in top 1
Epoch 110, global step 4440: 'val/CER' was not in top 1
Epoch 111, global step 4480: 'val/CER' was not in top 1
Epoch 112, global step 4520: 'val/CER' was not in top 1
Epoch 113, global step 4560: 'val/CER' was not in top 1
Epoch 114, global step 4600: 'val/CER' was not in top 1
Epoch 115, global step 4640: 'val/CER' was not in top 1
Epoch 116, global step 4680: 'val/CER' was not in top 1
Epoch 117, global step 4720: 'val/CER' was not in top 1
Epoch 118, global step 4760: 'val/CER' was not in top 1
Epoch 119, global step 4800: 'val/CER' was not in top 1
Epoch 120, global step 4840: 'val/CER' was not in top 1
Epoch 121, global step 4880: 'val/CER' was not in top 1
Epoch 122, global step 4920: 'val/CER' was not in top 1
Epoch 123, global step 4960: 'val/CER' was not in top 1
Epoch 124, global step 5000: 'val/CER' was not in top 1
Epoch 125, global step 5040: 'val/CER' was not in top 1
Epoch 126, global step 5080: 'val/CER' was not in top 1
Epoch 127, global step 5120: 'val/CER' was not in top 1
Epoch 128, global step 5160: 'val/CER' was not in top 1
Epoch 129, global step 5200: 'val/CER' was not in top 1
Epoch 130, global step 5240: 'val/CER' was not in top 1
Epoch 131, global step 5280: 'val/CER' was not in top 1
Epoch 132, global step 5320: 'val/CER' was not in top 1
Epoch 133, global step 5360: 'val/CER' was not in top 1
Epoch 134, global step 5400: 'val/CER' was not in top 1
Epoch 135, global step 5440: 'val/CER' was not in top 1
Epoch 136, global step 5480: 'val/CER' was not in top 1
Epoch 137, global step 5520: 'val/CER' was not in top 1
Epoch 138, global step 5560: 'val/CER' was not in top 1
Epoch 139, global step 5600: 'val/CER' was not in top 1
Epoch 140, global step 5640: 'val/CER' was not in top 1
Epoch 141, global step 5680: 'val/CER' was not in top 1
Epoch 142, global step 5720: 'val/CER' was not in top 1
Epoch 143, global step 5760: 'val/CER' was not in top 1
Epoch 144, global step 5800: 'val/CER' was not in top 1
Epoch 145, global step 5840: 'val/CER' was not in top 1
Epoch 146, global step 5880: 'val/CER' was not in top 1
Epoch 147, global step 5920: 'val/CER' was not in top 1
Epoch 148, global step 5960: 'val/CER' was not in top 1
Epoch 149, global step 6000: 'val/CER' was not in top 1
`Trainer.fit` stopped: `max_epochs=150` reached.
2025-03-09 20:10:18.027952: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Att2025-03-09 20:10:18.027952: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
empting to register factory for plugin cuFFT when one has already been registered
2025-03-09 20:10:18.032547: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741576218.039266  267051 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741576218.039266  267050 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741576218.042599  267051 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741576218.042599  267050 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741576218.043008  267052 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741576218.048486  267052 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-09 20:10:18.056547: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use avai2025-03-09 20:10:18.056548: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, lable CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
rebuild TensorFlow with the appropriate compiler flags.
2025-03-09 20:10:18.059361: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/3
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/3
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/3
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 3 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2]
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:315: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.validate()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
  rank_zero_warn(
/home/bkwan27/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/bkwan27/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/bkwan27/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
2025-03-09 20:10:34.176985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741576234.188111  268588 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741576234.191523  268588 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-09 20:10:34.203402: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-09 20:10:34.301857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741576234.312685  268589 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741576234.316021  268589 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-09 20:10:34.327712: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-09 20:10:34.381174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741576234.392080  268587 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741576234.395412  268587 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-09 20:10:34.407021: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/3
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/3
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/3
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 3 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:315: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
  rank_zero_warn(
/home/bkwan27/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/bkwan27/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/home/bkwan27/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
