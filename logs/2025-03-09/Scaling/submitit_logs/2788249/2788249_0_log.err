2025-03-09 22:59:27.552503: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741586367.573609 2788250 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741586367.580039 2788250 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-09 22:59:27.601587: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Global seed set to 1501
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2025-03-09 22:59:37.986671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741586378.007148 2788662 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741586378.013502 2788662 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-09 22:59:38.026522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-09 22:59:38.034382: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-09 22:59:38.043854: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741586378.046761 2788660 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741586378.053146 2788660 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741586378.064218 2788659 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-09 22:59:38.070592: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
E0000 00:00:1741586378.070692 2788659 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-09 22:59:38.073885: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741586378.091069 2788661 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-09 22:59:38.091655: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
E0000 00:00:1741586378.097405 2788661 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-09 22:59:38.118063: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Missing logger folder: /home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/lightning_logs
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

Missing logger folder: /home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/lightning_logs
Missing logger folder: /home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/lightning_logs
Missing logger folder: /home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/lightning_logs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.
  if not hasattr(numpy, tp_name):
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html
  "lr_options": generate_power_seq(LEARNING_RATE_CIFAR, 11),
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html
  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask("01, 02, 11"),
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html
  self.nce_loss = AmdimNCELoss(tclip)
/home/bkwan27/anaconda3/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html
  return _target_(*args, **kwargs)

  | Name     | Type       | Params
----------------------------------------
0 | model    | Sequential | 2.4 M 
1 | ctc_loss | CTCLoss    | 0     
2 | metrics  | ModuleDict | 0     
----------------------------------------
2.4 M     Trainable params
0         Non-trainable params
2.4 M     Total params
9.564     Total estimated model params size (MB)
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
/home/bkwan27/emg2qwerty/emg2qwerty/transforms.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  scale_factor = torch.tensor(0.8 + 0.4 * torch.rand(1))  # Random scale (0.8x to 1.2x)
[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Epoch 0, global step 30: 'val/CER' reached 119.14045 (best 119.14045), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=0-step=30.ckpt' as top 1
Epoch 1, global step 60: 'val/CER' reached 100.00000 (best 100.00000), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=1-step=60.ckpt' as top 1
Epoch 2, global step 90: 'val/CER' was not in top 1
Epoch 3, global step 120: 'val/CER' was not in top 1
Epoch 4, global step 150: 'val/CER' was not in top 1
Epoch 5, global step 180: 'val/CER' was not in top 1
Epoch 6, global step 210: 'val/CER' was not in top 1
Epoch 7, global step 240: 'val/CER' was not in top 1
Epoch 8, global step 270: 'val/CER' was not in top 1
Epoch 9, global step 300: 'val/CER' was not in top 1
Epoch 10, global step 330: 'val/CER' was not in top 1
Epoch 11, global step 360: 'val/CER' was not in top 1
Epoch 12, global step 390: 'val/CER' was not in top 1
Epoch 13, global step 420: 'val/CER' was not in top 1
Epoch 14, global step 450: 'val/CER' was not in top 1
Epoch 15, global step 480: 'val/CER' was not in top 1
Epoch 16, global step 510: 'val/CER' was not in top 1
Epoch 17, global step 540: 'val/CER' was not in top 1
Epoch 18, global step 570: 'val/CER' was not in top 1
Epoch 19, global step 600: 'val/CER' was not in top 1
Epoch 20, global step 630: 'val/CER' was not in top 1
Epoch 21, global step 660: 'val/CER' was not in top 1
Epoch 22, global step 690: 'val/CER' was not in top 1
Epoch 23, global step 720: 'val/CER' was not in top 1
Epoch 24, global step 750: 'val/CER' reached 99.55693 (best 99.55693), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=24-step=750.ckpt' as top 1
Epoch 25, global step 780: 'val/CER' reached 97.91759 (best 97.91759), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=25-step=780.ckpt' as top 1
Epoch 26, global step 810: 'val/CER' reached 93.53123 (best 93.53123), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=26-step=810.ckpt' as top 1
Epoch 27, global step 840: 'val/CER' reached 84.66991 (best 84.66991), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=27-step=840.ckpt' as top 1
Epoch 28, global step 870: 'val/CER' reached 83.45148 (best 83.45148), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=28-step=870.ckpt' as top 1
Epoch 29, global step 900: 'val/CER' reached 78.86575 (best 78.86575), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=29-step=900.ckpt' as top 1
Epoch 30, global step 930: 'val/CER' reached 66.50421 (best 66.50421), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=30-step=930.ckpt' as top 1
Epoch 31, global step 960: 'val/CER' reached 53.89898 (best 53.89898), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=31-step=960.ckpt' as top 1
Epoch 32, global step 990: 'val/CER' reached 52.21533 (best 52.21533), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=32-step=990.ckpt' as top 1
Epoch 33, global step 1020: 'val/CER' reached 48.82587 (best 48.82587), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=33-step=1020.ckpt' as top 1
Epoch 34, global step 1050: 'val/CER' reached 48.00620 (best 48.00620), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=34-step=1050.ckpt' as top 1
Epoch 35, global step 1080: 'val/CER' reached 45.12627 (best 45.12627), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=35-step=1080.ckpt' as top 1
Epoch 36, global step 1110: 'val/CER' reached 44.72752 (best 44.72752), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=36-step=1110.ckpt' as top 1
Epoch 37, global step 1140: 'val/CER' reached 42.97740 (best 42.97740), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=37-step=1140.ckpt' as top 1
Epoch 38, global step 1170: 'val/CER' was not in top 1
Epoch 39, global step 1200: 'val/CER' reached 41.29375 (best 41.29375), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=39-step=1200.ckpt' as top 1
Epoch 40, global step 1230: 'val/CER' reached 39.03411 (best 39.03411), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=40-step=1230.ckpt' as top 1
Epoch 41, global step 1260: 'val/CER' reached 38.36952 (best 38.36952), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=41-step=1260.ckpt' as top 1
Epoch 42, global step 1290: 'val/CER' reached 37.68277 (best 37.68277), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=42-step=1290.ckpt' as top 1
Epoch 43, global step 1320: 'val/CER' was not in top 1
Epoch 44, global step 1350: 'val/CER' reached 36.30926 (best 36.30926), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=44-step=1350.ckpt' as top 1
Epoch 45, global step 1380: 'val/CER' reached 34.27116 (best 34.27116), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=45-step=1380.ckpt' as top 1
Epoch 46, global step 1410: 'val/CER' reached 34.04962 (best 34.04962), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=46-step=1410.ckpt' as top 1
Epoch 47, global step 1440: 'val/CER' reached 33.80594 (best 33.80594), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=47-step=1440.ckpt' as top 1
Epoch 48, global step 1470: 'val/CER' reached 32.67612 (best 32.67612), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=48-step=1470.ckpt' as top 1
Epoch 49, global step 1500: 'val/CER' was not in top 1
Epoch 50, global step 1530: 'val/CER' was not in top 1
Epoch 51, global step 1560: 'val/CER' reached 29.97342 (best 29.97342), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=51-step=1560.ckpt' as top 1
Epoch 52, global step 1590: 'val/CER' was not in top 1
Epoch 53, global step 1620: 'val/CER' was not in top 1
Epoch 54, global step 1650: 'val/CER' reached 29.66327 (best 29.66327), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=54-step=1650.ckpt' as top 1
Epoch 55, global step 1680: 'val/CER' was not in top 1
Epoch 56, global step 1710: 'val/CER' was not in top 1
Epoch 57, global step 1740: 'val/CER' reached 29.55250 (best 29.55250), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=57-step=1740.ckpt' as top 1
Epoch 58, global step 1770: 'val/CER' reached 28.22330 (best 28.22330), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=58-step=1770.ckpt' as top 1
Epoch 59, global step 1800: 'val/CER' reached 28.13469 (best 28.13469), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=59-step=1800.ckpt' as top 1
Epoch 60, global step 1830: 'val/CER' reached 27.49225 (best 27.49225), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=60-step=1830.ckpt' as top 1
Epoch 61, global step 1860: 'val/CER' reached 27.47009 (best 27.47009), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=61-step=1860.ckpt' as top 1
Epoch 62, global step 1890: 'val/CER' was not in top 1
Epoch 63, global step 1920: 'val/CER' reached 25.69783 (best 25.69783), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=63-step=1920.ckpt' as top 1
Epoch 64, global step 1950: 'val/CER' was not in top 1
Epoch 65, global step 1980: 'val/CER' was not in top 1
Epoch 66, global step 2010: 'val/CER' reached 25.52060 (best 25.52060), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=66-step=2010.ckpt' as top 1
Epoch 67, global step 2040: 'val/CER' was not in top 1
Epoch 68, global step 2070: 'val/CER' was not in top 1
Epoch 69, global step 2100: 'val/CER' was not in top 1
Epoch 70, global step 2130: 'val/CER' was not in top 1
Epoch 71, global step 2160: 'val/CER' reached 25.18830 (best 25.18830), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=71-step=2160.ckpt' as top 1
Epoch 72, global step 2190: 'val/CER' was not in top 1
Epoch 73, global step 2220: 'val/CER' reached 24.85600 (best 24.85600), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=73-step=2220.ckpt' as top 1
Epoch 74, global step 2250: 'val/CER' reached 23.61542 (best 23.61542), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=74-step=2250.ckpt' as top 1
Epoch 75, global step 2280: 'val/CER' was not in top 1
Epoch 76, global step 2310: 'val/CER' was not in top 1
Epoch 77, global step 2340: 'val/CER' was not in top 1
Epoch 78, global step 2370: 'val/CER' reached 23.50465 (best 23.50465), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=78-step=2370.ckpt' as top 1
Epoch 79, global step 2400: 'val/CER' reached 23.15020 (best 23.15020), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=79-step=2400.ckpt' as top 1
Epoch 80, global step 2430: 'val/CER' reached 22.48560 (best 22.48560), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=80-step=2430.ckpt' as top 1
Epoch 81, global step 2460: 'val/CER' was not in top 1
Epoch 82, global step 2490: 'val/CER' was not in top 1
Epoch 83, global step 2520: 'val/CER' was not in top 1
Epoch 84, global step 2550: 'val/CER' was not in top 1
Epoch 85, global step 2580: 'val/CER' was not in top 1
Epoch 86, global step 2610: 'val/CER' was not in top 1
Epoch 87, global step 2640: 'val/CER' was not in top 1
Epoch 88, global step 2670: 'val/CER' reached 21.71024 (best 21.71024), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=88-step=2670.ckpt' as top 1
Epoch 89, global step 2700: 'val/CER' reached 21.68808 (best 21.68808), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=89-step=2700.ckpt' as top 1
Epoch 90, global step 2730: 'val/CER' was not in top 1
Epoch 91, global step 2760: 'val/CER' was not in top 1
Epoch 92, global step 2790: 'val/CER' was not in top 1
Epoch 93, global step 2820: 'val/CER' was not in top 1
Epoch 94, global step 2850: 'val/CER' was not in top 1
Epoch 95, global step 2880: 'val/CER' was not in top 1
Epoch 96, global step 2910: 'val/CER' was not in top 1
Epoch 97, global step 2940: 'val/CER' was not in top 1
Epoch 98, global step 2970: 'val/CER' reached 20.35888 (best 20.35888), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=98-step=2970.ckpt' as top 1
Epoch 99, global step 3000: 'val/CER' was not in top 1
Epoch 100, global step 3030: 'val/CER' was not in top 1
Epoch 101, global step 3060: 'val/CER' was not in top 1
Epoch 102, global step 3090: 'val/CER' reached 19.84936 (best 19.84936), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=102-step=3090.ckpt' as top 1
Epoch 103, global step 3120: 'val/CER' was not in top 1
Epoch 104, global step 3150: 'val/CER' was not in top 1
Epoch 105, global step 3180: 'val/CER' was not in top 1
Epoch 106, global step 3210: 'val/CER' reached 19.76074 (best 19.76074), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=106-step=3210.ckpt' as top 1
Epoch 107, global step 3240: 'val/CER' was not in top 1
Epoch 108, global step 3270: 'val/CER' was not in top 1
Epoch 109, global step 3300: 'val/CER' was not in top 1
Epoch 110, global step 3330: 'val/CER' was not in top 1
Epoch 111, global step 3360: 'val/CER' reached 19.58352 (best 19.58352), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=111-step=3360.ckpt' as top 1
Epoch 112, global step 3390: 'val/CER' was not in top 1
Epoch 113, global step 3420: 'val/CER' was not in top 1
Epoch 114, global step 3450: 'val/CER' was not in top 1
Epoch 115, global step 3480: 'val/CER' was not in top 1
Epoch 116, global step 3510: 'val/CER' was not in top 1
Epoch 117, global step 3540: 'val/CER' reached 19.56137 (best 19.56137), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=117-step=3540.ckpt' as top 1
Epoch 118, global step 3570: 'val/CER' was not in top 1
Epoch 119, global step 3600: 'val/CER' reached 19.36198 (best 19.36198), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=119-step=3600.ckpt' as top 1
Epoch 120, global step 3630: 'val/CER' reached 18.98538 (best 18.98538), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=120-step=3630.ckpt' as top 1
Epoch 121, global step 3660: 'val/CER' was not in top 1
Epoch 122, global step 3690: 'val/CER' was not in top 1
Epoch 123, global step 3720: 'val/CER' was not in top 1
Epoch 124, global step 3750: 'val/CER' was not in top 1
Epoch 125, global step 3780: 'val/CER' was not in top 1
Epoch 126, global step 3810: 'val/CER' was not in top 1
Epoch 127, global step 3840: 'val/CER' was not in top 1
Epoch 128, global step 3870: 'val/CER' was not in top 1
Epoch 129, global step 3900: 'val/CER' was not in top 1
Epoch 130, global step 3930: 'val/CER' was not in top 1
Epoch 131, global step 3960: 'val/CER' was not in top 1
Epoch 132, global step 3990: 'val/CER' was not in top 1
Epoch 133, global step 4020: 'val/CER' reached 18.65308 (best 18.65308), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=133-step=4020.ckpt' as top 1
Epoch 134, global step 4050: 'val/CER' was not in top 1
Epoch 135, global step 4080: 'val/CER' was not in top 1
Epoch 136, global step 4110: 'val/CER' reached 18.63093 (best 18.63093), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=136-step=4110.ckpt' as top 1
Epoch 137, global step 4140: 'val/CER' was not in top 1
Epoch 138, global step 4170: 'val/CER' reached 18.52016 (best 18.52016), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=138-step=4170.ckpt' as top 1
Epoch 139, global step 4200: 'val/CER' reached 18.38724 (best 18.38724), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=139-step=4200.ckpt' as top 1
Epoch 140, global step 4230: 'val/CER' was not in top 1
Epoch 141, global step 4260: 'val/CER' was not in top 1
Epoch 142, global step 4290: 'val/CER' was not in top 1
Epoch 143, global step 4320: 'val/CER' was not in top 1
Epoch 144, global step 4350: 'val/CER' was not in top 1
Epoch 145, global step 4380: 'val/CER' was not in top 1
Epoch 146, global step 4410: 'val/CER' was not in top 1
Epoch 147, global step 4440: 'val/CER' reached 18.25432 (best 18.25432), saving model to '/home/bkwan27/emg2qwerty/logs/2025-03-09/22-59-22/job0_trainer.devices=4,user=single_user/checkpoints/epoch=147-step=4440.ckpt' as top 1
Epoch 148, global step 4470: 'val/CER' was not in top 1
Epoch 149, global step 4500: 'val/CER' was not in top 1
`Trainer.fit` stopped: `max_epochs=150` reached.
2025-03-09 23:34:04.782127: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-09 23:34:04.782936: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-09 23:34:04.783154: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-09 23:34:04.783244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741588444.802538 2833753 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741588444.805723 2833754 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741588444.805734 2833755 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741588444.805732 2833756 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741588444.808903 2833753 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741588444.812076 2833756 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741588444.812249 2833755 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741588444.812267 2833754 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-09 23:34:04.829694: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-09 23:34:04.833463: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-09 23:34:04.835302: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-09 23:34:04.835455: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:315: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.validate()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
  rank_zero_warn(
2025-03-09 23:34:28.990612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741588469.011001 2838214 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741588469.017530 2838214 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-09 23:34:29.035088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-09 23:34:29.035510: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-09 23:34:29.038718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741588469.055611 2838212 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741588469.058598 2838213 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-09 23:34:29.060352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
E0000 00:00:1741588469.061942 2838212 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741588469.065171 2838213 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741588469.081744 2838211 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-09 23:34:29.084831: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-09 23:34:29.087049: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
E0000 00:00:1741588469.088105 2838211 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-09 23:34:29.111404: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
/home/bkwan27/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:315: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
  rank_zero_warn(
